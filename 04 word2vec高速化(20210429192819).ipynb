{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec的高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "啥也不说了，上高速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/bq2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec的改进1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先复习一下上一章的内容。在上一章中，我们实现了图中的CBOW 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设词汇量有 100 万个，CBOW 模型的中间层神经元有 100 个，此时 word2vec 进行的处理如图所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. H=WinX\n",
    "\n",
    "2. S = HWout\n",
    "\n",
    "3. Y = Softmax with loss（S）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重点关注这三个计算大户"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对计算大户1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我反应过来了，我们要的h，其实就是在Win中取出跟X对应的那一行即可\n",
    "\n",
    "那用不着dot，直接取就行了\n",
    "\n",
    "这个我们给他起名叫Embedding层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先看W中取出指定一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " W[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用索引就可以了so easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后从 W 中一次性提取多行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([1, 0, 3, 0])\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现Embedding层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers.py\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.arange(21).reshape(7, 3)\n",
    "embed = Embedding(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers.py\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params # 注意一下这个逗号，是把列表里面的值取出来 array\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.arange(21).reshape(7, 3)\n",
    "embed = Embedding(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, = embed.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.forward(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再看一下反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers.py\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params # 注意一下这个逗号，是把列表里面的值取出来 array\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        dW[self.idx] = dout # 不太好的方式\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.arange(21).reshape(7, 3)\n",
    "embed = Embedding(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.forward(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW, = embed.grads # 注意这里grads会跟着改变\n",
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW[...] = 0\n",
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW[embed.idx] = dout # 不太好的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.grads # 此时grads已经改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，取出权重梯度 dW，通过 dW[...] = 0 将 dW 的元素设为 0（并不是将 dW 设为 0，而是保持 dW 的形状不变，将它的元素设为 0）。\n",
    "\n",
    "然后，将上一层传来的梯度 dout 写入 idx 指定的行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有一个问题\n",
    "\n",
    "在 idx 的元素出现重复时。比如，当 idx 为 [0, 2, 0, 4] 时，会出现覆盖的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(W)\n",
    "embed.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.forward(np.array([0, 2, 0, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.backward(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0这个明明有两个，但是跟一个没去别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决这个重复问题，需要进行“加法”，而不是“写入”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers.py\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params # 注意一下这个逗号，是把列表里面的值取出来 array\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "#         for i, word_id in enumerate(self.idx):\n",
    "#             dW[word_id] += dout[i]\n",
    "#       或者\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(W)\n",
    "embed.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.forward(np.array([0, 2, 0, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导数的形状跟forward输出的形状一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = np.zeros((4,3), dtype=np.int32) # 注意这里必须定义32位\n",
    "dout[...] = int(3)\n",
    "dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.backward(dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里可以看到，0位置的梯度被累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，这里使用 for 循环语句的实现也可以通过 NumPy 的 np.add.at() 进行。\n",
    "\n",
    "np.add.at(A, idx, B) 将 B 加到 A 上，此时可以通过 idx 指定 A 中需要进行加法的行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <strong>冷静</strong> \n",
    "考虑一下为什么是加法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/tuidao4-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec的改进2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. H=WinX\n",
    "\n",
    "2. S = HWout\n",
    "\n",
    "3. Y = Softmax with loss（S）\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1已解决，下面是问题2和问题3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中间层之后的计算问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. S = HWout\n",
    "\n",
    "3. Y = Softmax with loss（S）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，问题2与问题3的计算量都跟词汇量相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们要实现一种方法，无论词汇量多大，都能使得计算量保持较低或者恒定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从多分类到二分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑用二分类拟合多分类\n",
    "\n",
    "多分类：给you和goodbye，目标词是谁？\n",
    "\n",
    "二分类：“当上下文是 you 和 goodbye 时，目标词是 say 吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么我们的计算就变成下图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid函数和交叉熵误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二分类用sigmoid拟合概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向求导如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数交叉熵误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = 1时，-logy\n",
    "\n",
    "t = 0时， -log(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 与 CrossEntropy Error合成一个层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个推导上本书都推过了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多分类到二分类的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾一下，多分类的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改成二分类的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二分类这个快合并一个Embedding Dot层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现这个层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W) # 这里生成了Embedding层\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- params 保存参数\n",
    "\n",
    "- grads 保存梯度\n",
    "\n",
    "- 另外，作为缓存，embed 保存 Embedding 层\n",
    "\n",
    "- cache 保存正向传播时的计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wout = np.arange(21).reshape(7, 3)\n",
    "embeddot = EmbeddingDot(Wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddot.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddot.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddot.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch04/negative_sampling_layer.py\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W) # 这里生成了Embedding层\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx) # Wout对应的列\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "        self.cache = (h, target_W)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out = np.sum(target_W * h, axis=1) 是怎么回事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/bq17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对这里  的解释，书上给了这么一张图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "书上的文字稍微有点困惑，我是这样理解的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/tuidao4-3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.arange(21).reshape(7, 3)\n",
    "embeddot = EmbeddingDot(W)\n",
    "idx = np.array([1,2,4])\n",
    "target_W = embeddot.embed.forward(idx)\n",
    "target_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.array([[3,1,2], [3,1,2], [3,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_W * h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddot.forward(h, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/bq9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W) # 这里生成了Embedding层\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx) # Wout对应的列\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.arange(21).reshape(7, 3)\n",
    "embeddot = EmbeddingDot(W)\n",
    "embeddot.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([1,2,4])\n",
    "h = np.array([[3,1,2], [3,1,2], [3,1,2]])\n",
    "embeddot.forward(h, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, target_W = embeddot.cache\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = np.array([4, 3, 2])\n",
    "dout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = dout.reshape(dout.shape[0], 1)\n",
    "dout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtarget_W = dout * h\n",
    "dtarget_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddot.embed.backward(dtarget_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddot.embed.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dout * target_W\n",
    "dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = embeddot.backward(dout)\n",
    "dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里反向上游的梯度，对应h广播后的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何采样呢，之前只考虑了say，正确解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时还要考虑若干个负例\n",
    "\n",
    "![](./img/4-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要对所有的负例进行学习吗？\n",
    "\n",
    "Non non non，pas du tout~\n",
    "\n",
    "我们只需要选几个个负例即可\n",
    "\n",
    "比如下面这张图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即Loss = Loss(say) + Loss(hello) + Loss(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么这几个负例如何选择呢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负采样层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采样有很多方法，比如\n",
    "\n",
    "- 随机抽样\n",
    "\n",
    "- 基于语料库的统计抽样\n",
    "\n",
    "这里我们选择第二种\n",
    "\n",
    "高频词汇抽到的概率大，低频词汇抽到的概率小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下python怎么实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 从0到9的数字中随机选择一个数字\n",
    "np.random.choice(10) # 注意这个用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从words列表中随机选择一个元素\n",
    "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有放回采样5次\n",
    "np.random.choice(words, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无放回采样5次\n",
    "np.random.choice(words, size=5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于概率分布进行采样\n",
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "np.random.choice(words, p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于我们的情况，假设知道有5个单词，以p为概率，取两个出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "np.random.choice(6, size=2, replace=False, p=p) # 取出索引即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外对概率做一个平滑处理\n",
    "\n",
    "分softmax有点类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个0.75没有理论依据，随便选的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0.7, 0.29, 0.01]\n",
    "new_p = np.power(p, 0.75)\n",
    "new_p /= np.sum(new_p)\n",
    "print(new_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下代码怎么实现，先看基本的概率计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3]) # 单词序列对应索引\n",
    "power = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = collections.Counter()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_id in corpus:\n",
    "    counts[word_id] += 1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(counts)\n",
    "vocab_size # 一共有五个单词，索引0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_p = np.zeros(vocab_size)\n",
    "word_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(vocab_size):\n",
    "    word_p[i] = counts[i]\n",
    "word_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_p = np.power(word_p, power)\n",
    "word_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_p /= np.sum(word_p)\n",
    "word_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch04/negative_sampling_layer.py\n",
    "import collections\n",
    "from common.np import *  # import numpy as np\n",
    "\n",
    "class UnigramSampler:\n",
    "    \n",
    "    # 这段这个很简单\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    # \n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        if not GPU:\n",
    "            # 这里的batch_size应该是取样\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy() # 每个单词选到的概率\n",
    "                target_idx = target[i] # 正例\n",
    "                p[target_idx] = 0 # 这两让正例的概率为零\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # 在用GPU(cupy）计算时，优先速度\n",
    "            # 有时目标词存在于负例中\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后是get_negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，考虑将 [1, 3, 0] 这 3 个数据的 mini-batch 作为正例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array([1, 3, 0])\n",
    "batch_size = target.shape[0]\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 2\n",
    "# 这里的batch_size应该是取样\n",
    "negative_sample = np.zeros((batch_size, sample_size), dtype=np.int32)\n",
    "negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，对各个数据采样 2 个负例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size):\n",
    "    # 每个target取一次\n",
    "    p = word_p.copy() # 每个单词选到的概率\n",
    "    target_idx = target[i] # 正例\n",
    "    p[target_idx] = 0 # 这两让正例的概率为零\n",
    "    p /= p.sum() # 重新计算分布\n",
    "    negative_sample[i, :] = np.random.choice(vocab_size, size=sample_size, replace=False, p=p)\n",
    "    # 以p的概率，从0-4的索引中，无放回抽样，取出两个\n",
    "negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可知第 1 个数据的负例是 [0, 3]， 第 2 个是 [1, 2]，第 3 个是 [2, 3]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装为class就是上面那个\n",
    "\n",
    "使用 UnigramSampler这个名字，是因为我们以 1 个单词为对象创建概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行初始化时，UnigramSampler 类取 3 个参数，\n",
    "\n",
    "- 分别是单词 ID 列表格式的 corpus\n",
    "\n",
    "- 对概率分布取的次方值 power（默认值是 0.75）\n",
    "\n",
    "- 负例的采样个数 sample_size。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UnigramSampler 类有 get_negative_sample(target) 方法\n",
    "\n",
    "target 指定的单词 ID 为正例，对其他的单词 ID 进行采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外这里看一下GPU的情况，这里为了速度，就忽略正例了\n",
    "\n",
    "负采样中可能包含正例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负采样+损失层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "听说这里有点绕啊，我们一起来看一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/bq12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import *\n",
    "W = np.arange(21).reshape(7, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power=0.75\n",
    "sample_size= 4 # 每笔数据，负样例个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = UnigramSampler(corpus, power, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)] # 负样例2个+正样例1个\n",
    "loss_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)] # 负样例2个+正样例1个\n",
    "embed_dot_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意loss_layers[0] 和 embed_dot_layers[0] 是处理正例的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数与对应的导数，参数是对应三个层的参数列表，导数也是对应三个层导数的列表\n",
    "params, grads = [], []\n",
    "for layer in embed_dot_layers:\n",
    "    params += layer.params\n",
    "    grads += layer.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch04/negative_sampling_layer.py\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size\n",
    "        + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in\n",
    "        range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward\n",
    "\n",
    "输入h和target，输入损失loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = target.shape[0]\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sample = sampler.get_negative_sample(target)\n",
    "negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正例的正向传播 就两步，embed_dot，完了loss\n",
    "score = embed_dot_layers[0].forward(h, target)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_layers[0].forward(score, correct_label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负例的正向传播\n",
    "negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "negative_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # 一共四个负例，编号0-3，对于每笔数据的第一个负样例\n",
    "negative_target = negative_sample[:, i]\n",
    "negative_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "score # 得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算损失\n",
    "loss += loss_layers[1 + i].forward(score, negative_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个笔数据，都按顺序取出一个负例进行一次损失计算\n",
    "for i in range(sample_size):\n",
    "    negative_target = negative_sample[:, i]\n",
    "    score = embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "    loss += loss_layers[1 + i].forward(score, negative_label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch04/negative_sampling_layer.py\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size\n",
    "        + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in\n",
    "        range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, h, target): # 中间层的神经元 h 和正例目标词 target\n",
    "        batch_size = target.shape[0]\n",
    "        # 首先使用 self.sampler 采样负例，并设为negative_sample\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        # 分别对正例和负例的数据进行正向传播，求损失的和\n",
    "        \n",
    "        # 正例的正向传播\n",
    "        ## 通过 Embedding Dot 层的 forward 输出得分\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        ## 再将这个得分和标签一起输入 Sigmoid with Loss 层来计算损失\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32) # 正例的正确解标签为 1\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        # 负例的正向传播\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32) # 负例的正确解标签为 0\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在正向传播时，中间层的神经元被复制了多份，这相 Repeat 节点。\n",
    "\n",
    "在反向传播时，需要将多份梯度累加起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch04/negative_sampling_layer.py\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size\n",
    "        + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in\n",
    "        range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        # 正例的正向传播\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        # 负例的正向传播\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout) # 对于每一笔数据，先传loss\n",
    "            dh += l1.backward(dscore) # 在传embed_dot\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/bq13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进版word3vec的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PTB 数据集上进行学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW模型的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们将改进上一章的简单的 SimpleCBOW类，来实现 CBOW 模型。\n",
    "改进之处在于使用 Embedding 层和 Negative Sampling Loss 层。此外，我\n",
    "们将上下文部分扩展为可以处理任意的窗口大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch04/cbow.py\n",
    "from common.np import *  # import numpy as np\n",
    "# from common.layers import Embedding\n",
    "# from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "\n",
    "        # vocab_size 是词汇量，hidden_size 是中间层的神经元个数，corpus 是单词 ID 列表。\n",
    "        # 另外，通过 window_size 指定上下文的大小，即上下文包含多少个周围单词。\n",
    "        # 如果 window_size 是 2，则目标词的左右 2 个单词（共 4 个单词）将成为上下文。\n",
    "        \n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 初始化权重\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # 生成层\n",
    "        self.in_layers = []\n",
    "        # 这里，创建 2 * window_size 个Embedding 层，并将其保存在成员变量 in_layers 中\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # 使用Embedding层\n",
    "            self.in_layers.append(layer)\n",
    "        # 然后，创建 Negative Sampling Loss 层\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # 将所有的权重和梯度整理到列表中\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        \n",
    "        # 在创建好层之后，将神经网络中使用的参数和梯度放入成员变量 params和 grads 中\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 将单词的分布式表示设置为成员变量，为了之后可以访问单词的分布式表示\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleCBOW类（改进前的实现）中，输入侧的权重和输出侧的权重的形状不同，输出侧的权重在列方向上排列单词向量。\n",
    "\n",
    "而CBOW类的输出侧的权重和输入侧的权重形状相同，都在行方向上排列单词向量。\n",
    "\n",
    "这是因为 NegativeSamplingLoss类中使用了Embedding 层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的实现只是按适当的顺序调用各个层的正向传播（或反向传播），这是对上一章的 SimpleCBOW 类的自然扩展。\n",
    "\n",
    "不过，虽然 forward (contexts, target) 方法取的参数仍是上下文和目标词，但是它们是单词 ID 形式的（上\n",
    "一章中使用的是 one-hot 向量，不是单词 ID），具体示例如图所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "右侧显示的单词 ID 列表是 contexts 和 target 的例子。可以\n",
    "看出，contexts 是一个二维数组，target 是一个一维数组，这样的数据被输\n",
    "入 forward(contexts, target) 中。以上就是 CBOW 类的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW模型学习的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch04/train.py\n",
    "from common import config\n",
    "from common.np import *\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from ch04.cbow import CBOW\n",
    "from ch04.skip_gram import SkipGram\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定超参数\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "# config.GPU = False\n",
    "config.GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次的 CBOW 模型的窗口大小为 5，隐藏层的神经元个数为 100。\n",
    "\n",
    "虽然具体取决于语料库的情况，但是一般而言，当窗口大小为 2 ～ 10、中间层的神经元个数（单词的分布式表示的维数）为50～500时，结果会比较好。\n",
    "\n",
    "稍后我们会对这些超参数进行讨论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\06Jupyter\\\\03_code\\\\18深度学习进阶自然语言处理\\\\神仔的代码\\\\dataset'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset/ptb.py\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "key_file = {\n",
    "    'train':'ptb.train.txt',\n",
    "    'test':'ptb.test.txt',\n",
    "    'valid':'ptb.valid.txt'\n",
    "}\n",
    "save_file = {\n",
    "    'train':'ptb.train.npy',\n",
    "    'test':'ptb.test.npy',\n",
    "    'valid':'ptb.valid.npy'\n",
    "}\n",
    "vocab_file = 'ptb.vocab.pkl'\n",
    "\n",
    "dataset_dir = os.path.join(os.path.abspath('.'), 'dataset')\n",
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            word_to_id, id_to_word = pickle.load(f)\n",
    "        return word_to_id, id_to_word\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type='train'):\n",
    "    '''\n",
    "        :param data_type: 数据的种类：'train' or 'test' or 'valid (val)'\n",
    "        :return:\n",
    "    '''\n",
    "    if data_type == 'val': data_type = 'valid'\n",
    "    save_path = dataset_dir + '/' + save_file[data_type]\n",
    "\n",
    "    word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        corpus = np.load(save_path)\n",
    "        return corpus, word_to_id, id_to_word\n",
    "\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = dataset_dir + '/' + file_name\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    np.save(save_path, corpus)\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "corpus, word_to_id, id_to_word = load_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    print(1)\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成模型等\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 9295 | time 0[s] | loss 4.16\n",
      "| epoch 1 |  iter 21 / 9295 | time 2[s] | loss 4.16\n",
      "| epoch 1 |  iter 41 / 9295 | time 4[s] | loss 4.15\n",
      "| epoch 1 |  iter 61 / 9295 | time 6[s] | loss 4.12\n",
      "| epoch 1 |  iter 81 / 9295 | time 9[s] | loss 4.04\n",
      "| epoch 1 |  iter 101 / 9295 | time 11[s] | loss 3.93\n",
      "| epoch 1 |  iter 121 / 9295 | time 13[s] | loss 3.77\n",
      "| epoch 1 |  iter 141 / 9295 | time 16[s] | loss 3.63\n",
      "| epoch 1 |  iter 161 / 9295 | time 18[s] | loss 3.48\n",
      "| epoch 1 |  iter 181 / 9295 | time 20[s] | loss 3.36\n",
      "| epoch 1 |  iter 201 / 9295 | time 23[s] | loss 3.23\n",
      "| epoch 1 |  iter 221 / 9295 | time 26[s] | loss 3.17\n",
      "| epoch 1 |  iter 241 / 9295 | time 29[s] | loss 3.08\n",
      "| epoch 1 |  iter 261 / 9295 | time 31[s] | loss 3.01\n",
      "| epoch 1 |  iter 281 / 9295 | time 34[s] | loss 2.95\n",
      "| epoch 1 |  iter 301 / 9295 | time 36[s] | loss 2.92\n",
      "| epoch 1 |  iter 321 / 9295 | time 39[s] | loss 2.87\n",
      "| epoch 1 |  iter 341 / 9295 | time 42[s] | loss 2.85\n",
      "| epoch 1 |  iter 361 / 9295 | time 44[s] | loss 2.83\n",
      "| epoch 1 |  iter 381 / 9295 | time 47[s] | loss 2.79\n",
      "| epoch 1 |  iter 401 / 9295 | time 49[s] | loss 2.76\n",
      "| epoch 1 |  iter 421 / 9295 | time 51[s] | loss 2.74\n",
      "| epoch 1 |  iter 441 / 9295 | time 53[s] | loss 2.73\n",
      "| epoch 1 |  iter 461 / 9295 | time 55[s] | loss 2.72\n",
      "| epoch 1 |  iter 481 / 9295 | time 57[s] | loss 2.70\n",
      "| epoch 1 |  iter 501 / 9295 | time 59[s] | loss 2.68\n",
      "| epoch 1 |  iter 521 / 9295 | time 62[s] | loss 2.70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-79a12c6ff1c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 开始学习\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\06Jupyter\\03_code\\18深度学习进阶自然语言处理\\神仔的代码\\common\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m# 计算梯度，更新参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 将共享的权重整合为1个\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\06Jupyter\\03_code\\18深度学习进阶自然语言处理\\神仔的代码\\ch04\\cbow.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, contexts, target)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mns_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\06Jupyter\\03_code\\18深度学习进阶自然语言处理\\神仔的代码\\ch04\\negative_sampling_layer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, h, target)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mnegative_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_negative_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# 正例的正向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\06Jupyter\\03_code\\18深度学习进阶自然语言处理\\神仔的代码\\ch04\\negative_sampling_layer.py\u001b[0m in \u001b[0;36mget_negative_sample\u001b[1;34m(self, target)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mnegative_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;31m# 在用GPU(cupy）计算时，优先速度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcount_nonzero\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始学习\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行起来非常慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这次我们利用的 PTB 语料库比之前要大得多，因此学习需要很长时间（半天左右）。\n",
    "\n",
    "作为一种选择，我们提供了使用 GPU 运行的模式。如果要使用 GPU 运行，需要打开顶部的“# config.GPU = True”。\n",
    "\n",
    "不过，使用 GPU运行需要有一台安装了 NVIDIA GPU 和 CuPy 的机器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存必要数据，以便后续使用\n",
    "word_vecs = model.word_vecs\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在学习结束后，取出权重（输入侧的权重），并保存在文件中以备后用（用于单词和单词 ID 之间的转化的字典也一起保存）。\n",
    "\n",
    "这里，使用 Python的 pickle 功能进行文件保存。pickle 可以将 Python 代码中的对象保存到文件中（或者从文件中读取对象）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ch04/cbow_params.pkl中提供了学习好的参数。\n",
    "\n",
    "如果不想等学习结束，可以使用本书提供的学习好的参数。\n",
    "\n",
    "根据学习环境的不同，学习到的权重数据也不一样。\n",
    "\n",
    "这是由权重初始化时用到的随机初始值、mini-bath 的随机选取，以及负采样的随机抽样造成的。\n",
    "\n",
    "因为这些随机性，最后得到的权重在各自的环境中会不一样。\n",
    "\n",
    "不过宏观来看，得到的结果（趋势）是类似的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 0px\">\n",
    "    \n",
    "**注意！！**\n",
    "\n",
    "如果使用GPU则需要修改代码\n",
    "\n",
    "https://www.ituring.com.cn/book/2678#reply-item-33298\n",
    "\n",
    "https://docs.cupy.dev/en/stable/upgrade.html#cupy-v7\n",
    "    \n",
    "https://crieit.net/posts/module-cupy-has-no-attribute-scatter-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/bq15.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COBW模型的评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们来评价一下上一节学习到的单词的分布式表示。这里我们\n",
    "使用第 2 章中实现的 most_similar() 函数，显示几个单词的最接近的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch04/eval.py\n",
    "from common.util import most_similar, analogy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = './ch04/cbow_params.pkl' # 作者训练的\n",
    "# pkl_file = 'cbow_params.pkl' # 我训练的\n",
    "# pkl_file = 'skipgram_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，在查询 you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.74609375\n",
      " i: 0.6748046875\n",
      " your: 0.6455078125\n",
      " they: 0.60595703125\n",
      " weird: 0.58740234375\n"
     ]
    }
   ],
   "source": [
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "most_similar(querys[0], word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近似单词中出现了人称代词 i（= I）和 we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] year\n",
      " month: 0.83447265625\n",
      " week: 0.77978515625\n",
      " summer: 0.76220703125\n",
      " spring: 0.74365234375\n",
      " decade: 0.71728515625\n"
     ]
    }
   ],
   "source": [
    "most_similar(querys[1], word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] car\n",
      " window: 0.599609375\n",
      " auto: 0.572265625\n",
      " luxury: 0.564453125\n",
      " vehicle: 0.55419921875\n",
      " truck: 0.55029296875\n"
     ]
    }
   ],
   "source": [
    "most_similar(querys[2], word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，查询 year，可以看到 month、week 等表示时间区间的具有相同性质的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] toyota\n",
      " honda: 0.63134765625\n",
      " digital: 0.62548828125\n",
      " engines: 0.615234375\n",
      " coated: 0.6064453125\n",
      " chevrolet: 0.60302734375\n"
     ]
    }
   ],
   "source": [
    "most_similar(querys[3], word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，查询 toyota，可以得到 ford、mazda 和 nissan 等表示汽车制造商的词汇。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从这些结果可以看出，由CBOW 模型获得的单词的分布式表示具有良好的性质。\n",
    "\n",
    "此外，由 word2vec 获得的单词的分布式表示不仅可以将近似单词聚拢在一起，还可以捕获更复杂的模式\n",
    "\n",
    "其中一个具有代表性的例子是因“king − man + woman = queen”而出名的类推问题（类比问题）。\n",
    "\n",
    "使用 word2vec 的单词的分布式表示，可以通过向量的加减法来解决类推问题。\n",
    "\n",
    "需要在单词向量空间上寻找尽可能使“man → woman”向量和“king → ?”向量接近的单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用 vec(‘man’) 表示单词 man 的分布式表示（单词向量）。如此一来，\n",
    "图 4-20 中要求的关联性可以用数学式表示为 vec(‘woman’) − vec(‘man’) = \n",
    "vec(?) − vec(‘king’)。将其变形，有 vec(‘king’) + vec(‘woman’) − vec(‘man’) = \n",
    "vec(?)。也就是说，我们的任务是找到离向量 vec(‘king’) + vec(‘woman’) −\n",
    "vec(‘man’) 最近的单词向量。本书在 common/util.py 中提供了实现此逻辑\n",
    "的函数 analogy()。使用这个函数，可以用 analogy('man', 'king', 'woman', \n",
    "word_to_id, id_to_word, word_vecs, top=5) 这样 1 行代码来回答刚才的类推\n",
    "问题。此时，输出的结果如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] king:man = queen:?\n",
      " woman: 5.4609375\n",
      " a.m: 5.12109375\n",
      " text: 4.90234375\n",
      " yard: 4.8671875\n",
      " daughter: 4.7734375\n"
     ]
    }
   ],
   "source": [
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第 1 个问题是“king : man = queen : ?”，这里正确地回答了“woman”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] take:took = go:?\n",
      " eurodollars: 4.7421875\n",
      " 're: 4.38671875\n",
      " came: 4.27734375\n",
      " was: 4.2265625\n",
      " went: 4.21484375\n"
     ]
    }
   ],
   "source": [
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第 2 个问题是“take : took = go : ?”，也按预期回答了“went”。\n",
    "\n",
    "这是捕获了现在时和过去时之间的模式的证据，可以解释为单词的分布式表示编码了时态相关的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] car:cars = child:?\n",
      " a.m: 5.91796875\n",
      " rape: 5.359375\n",
      " children: 5.19140625\n",
      " incest: 4.85546875\n",
      " bond-equivalent: 4.78515625\n"
     ]
    }
   ],
   "source": [
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从第 3 题可知，单词的单数形式和复数形式之间的模式也被正确地捕获。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] good:better = bad:?\n",
      " rather: 6.21484375\n",
      " more: 5.99609375\n",
      " less: 5.52734375\n",
      " greater: 4.390625\n",
      " faster: 4.25\n"
     ]
    }
   ],
   "source": [
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可惜的是，对于第 4 题“good : better = bad : ?”，并没能回答出“worse”。\n",
    "\n",
    "不过，看到 more、less 等比较级的单词出现在回答中，说明这些性质也被编码在了单词的分布式表示中。\n",
    "\n",
    "像这样，使用 word2vec 获得的单词的分布式表示，可以通过向量的加减法求解类推问题。不仅限于单词的含义，它也捕获了语法中的模式。\n",
    "\n",
    "另外，我们还在 word2vec 的单词的分布式表示中发现了一些有趣的结果，比如 good 和 best 之间存在 better 这样的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/bq10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的类推问题的结果看上去非常好。\n",
    "\n",
    "不过遗憾的是，这是笔者特意选出来的能够被顺利解决的问题。\n",
    "\n",
    "实际上，很多问题都无法获得预期的结果。\n",
    "\n",
    "这是因为 PTB 数据集的规模还是比较小。\n",
    "\n",
    "如果使用更大规模的语料库，可以获得更准确、更可靠的单词的分布式表示，从而大大提高类推问题的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordvec相关的其他话题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec的应用例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务流程\n",
    "\n",
    "在解决自然语言处理任务时，一般不会使用 word2vec 从零开始学习单词的分布式表示，而是先在大规模语料库（Wikipedia、Google News 等文本数据）上学习，然后将学习好的分布式表示应用于某个单独的任务。\n",
    "\n",
    "比如，在文本分类、文本聚类、词性标注和情感分析等自然语言处理任务中，第一步的单词向量化工作就可以使用学习好的单词的分布式表示。\n",
    "\n",
    "在几乎所有类型的自然语言处理任务中，单词的分布式表示都有很好的效果！\n",
    "\n",
    "单词的分布式表示的学习和机器学习系统的学习通常使用不同的数据集独立进行。\n",
    "\n",
    "比如，单词的分布式表示使用Wikipedia 等通用语料库预先学习好，然后机器学习系统（SVM 等）再使用针对当前问题收集到的数据进行学习。\n",
    "\n",
    "但是，如果当前我们面对的问题存在大量的学习数据，则也可以考虑从零开始同时进行单词的分布式表示和机器学习系统的学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 也可以将文档（单词序列）转化为固定长度的向量。\n",
    "\n",
    "目前，关于如何将文档转化为固定长度的向量，相关研究已经进行了很多，最简单的方法是，把文档的各个单词转化为分布式表示，然后求它们的总和。\n",
    "\n",
    "这是一种被称为 bag-of-words 的不考虑单词顺序的模型（思想）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比如对用户发来的邮件（吐槽等）自动进行分类的系统。\n",
    "\n",
    "你想根据邮件的内容将用户情感分为 3 类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要开发邮件自动分类系统，首先需要从收集数据（邮件）开始。\n",
    "\n",
    "我们收集用户发送的邮件，并人工对邮件进行标注，打上表示 3类情感的标签（positive/neutral/negative）。\n",
    "\n",
    "标注工作结束后，用学习好的word2vec 将邮件转化为向量。\n",
    "\n",
    "然后，将向量化的邮件及其情感标签输入某个情感分类系统（SVM 或神经网络等）进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单词向量的评价方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单词的分布式表示的学习和分类系统的学习有时可能会分开进行\n",
    "\n",
    "如果整个过程作为一个系统统一评价，比如要调查单词的分布式表示的维数如何影响最终的精度\n",
    "\n",
    "- 首先需要进行单词的分布式表示的学习\n",
    "\n",
    "- 然后再利用这个分布式表示进行另一个机器学习系统的学习。\n",
    "\n",
    "在这种情况下，由于需要调试出对两个系统都最优的超参数，所以非常费时。\n",
    "\n",
    "\n",
    "因此，单词的分布式表示的评价往往与实际应用分开进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单词的分布式表示评价\n",
    "\n",
    "此时，经常使用的评价指标有“相似度”和“类推问题”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单词相似度的评价通常使用人工创建的单词相似度评价集来评估。\n",
    "\n",
    "比如，cat 和 animal 的相似度是 8，cat 和 car 的相似度是 2……类似这样，\n",
    "\n",
    "用 0 ～ 10 的分数人工地对单词之间的相似度打分。\n",
    "\n",
    "然后，比较人给出的分数和 word2vec 给出的余弦相似度，考察它们之间的相关性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类推问题的评价是指，\n",
    "\n",
    "基于诸如“king : queen = man : ?”这样的类\n",
    "推问题，\n",
    "\n",
    "根据正确率测量单词的分布式表示的优劣。\n",
    "\n",
    "比如，论文 [27] 中给出了一个类推问题的评价结果，其部分内容如图 4-23 所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验变量\n",
    "\n",
    "- word2vec 的模型\n",
    "\n",
    "- 单词的分布式表示的维数\n",
    "\n",
    "- 语料库的大小为参数进行了比较实验\n",
    "\n",
    "比较指标\n",
    "\n",
    "- Semantics列显示的是推断单词含义的类推问题（像“king : queen = actor : actress”这样询问单词含义的问题）的正确率\n",
    "\n",
    "- Syntax 列是询问单词形态信息的问题，比如“bad : worst = good : best”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论\n",
    "\n",
    "- 模型不同，精度不同（根据语料库选择最佳的模型）\n",
    "\n",
    "- 语料库越大，结果越好（始终需要大数据）\n",
    "\n",
    "- 单词向量的维数必须适中（太大会导致精度变差）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于类推问题可以在一定程度上衡量“是否正确理解了单词含义或语法问题”。\n",
    "\n",
    "因此，在自然语言处理的应用中，能够高精度地解决类推问题的单词的分布式表示应该可以获得好的结果。\n",
    "\n",
    "但是，单词的分布式表示的优劣对目标应用贡献多少（或者有无贡献），取决于待处理问题的具体情况，比如应用的类型或语料库的内容等。\n",
    "\n",
    "也就是说，不能保证类推问题的评价高，目标应用的结果就一定好。这一点请一定注意。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一点点加牛栏山，哈哈，精神又暖和~~容光焕发"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感谢群里小伙伴的提问与互动，支持着我把这里看完，嘻嘻"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180.175px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
